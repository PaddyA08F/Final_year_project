{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d82e1a8530>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from numpy import log \n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generalisable NN\n",
    "torch.manual_seed(32)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, layer_dim, num_hidden, reg_param, reg_power, ran_seed):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define size of input and output layers\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define size of hidden layers (number of neurons)\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Define how many hidden layers\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        # Define activation function of neurons \n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        # Define regularisation parameter\n",
    "        self.reg_param = reg_param\n",
    "\n",
    "        # Defin regularisation exponent power\n",
    "        self.reg_power = reg_power\n",
    "\n",
    "        # Define random seed to be used\n",
    "        self.ran_seed = ran_seed\n",
    "\n",
    "        # Build architectyre\n",
    "        self.input_layer = nn.Linear(self.input_dim, self.layer_dim)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(self.layer_dim, self.layer_dim) for _ in range(num_hidden-1)])\n",
    "        self.output_layer = nn.Linear(self.layer_dim, self.output_dim)\n",
    "        self.layers = nn.ModuleList([self.input_layer, *self.hidden_layers, self.output_layer])\n",
    "\n",
    "        # Initialise weights\n",
    "        self.initialise()\n",
    "\n",
    "    def forward(self, x):\n",
    "    # Function defines the forward pass of the network\n",
    "        for _, f in enumerate(self.layers[:-1]):\n",
    "            x = self.activation(f(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def initialise(self):\n",
    "    # Function defines initilaising the unfrozen weights of the network using Xavier (Glorot) initialisiation\n",
    "        torch.manual_seed(self.ran_seed)\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Linear and m.weight.requires_grad and m.bias.requires_grad:\n",
    "                g = nn.init.calculate_gain('tanh')\n",
    "                #torch.nn.init.xavier_uniform_(m.weight, gain=g)\n",
    "                torch.nn.init.xavier_normal_(m.weight, gain=g)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def regularization(self):\n",
    "    # Function defines regularisation\n",
    "        reg_loss = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                reg_loss = reg_loss + torch.norm(param, self.reg_power)\n",
    "        return self.reg_param * reg_loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "exp(): argument 'input' (position 1) must be Tensor, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 196\u001b[0m\n\u001b[0;32m    193\u001b[0m alpha_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m    195\u001b[0m dist_bound \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0.0\u001b[39m], [\u001b[38;5;241m10.0\u001b[39m]])  \u001b[38;5;66;03m# Slab distance from 0 to 10\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m int_bound \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[I0], [emission_value\u001b[38;5;241m/\u001b[39malpha_value \u001b[38;5;241m+\u001b[39m(I0 \u001b[38;5;241m-\u001b[39m emission_value\u001b[38;5;241m/\u001b[39malpha_value)\u001b[38;5;241m*\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43malpha_value\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m]])  \u001b[38;5;66;03m# Intensity at boundaries\u001b[39;00m\n\u001b[0;32m    198\u001b[0m x_min \u001b[38;5;241m=\u001b[39m dist_bound\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    199\u001b[0m x_max \u001b[38;5;241m=\u001b[39m dist_bound\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mTypeError\u001b[0m: exp(): argument 'input' (position 1) must be Tensor, not float"
     ]
    }
   ],
   "source": [
    "# Define the PINN class\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, balancing_parameter, input_dim, output_dim, layer_dim, num_hidden, reg_param, reg_power, ran_seed):\n",
    "        super(PINN, self).__init__()\n",
    "\n",
    "        # Initialize the NeuralNetwork for intensity and absorption coefficient\n",
    "        self.intensity_network = NeuralNetwork(input_dim, output_dim, layer_dim, num_hidden, reg_param, reg_power, ran_seed)\n",
    "        self.emission_network = NeuralNetwork(input_dim, output_dim, layer_dim, num_hidden, reg_param, reg_power, ran_seed)\n",
    "        self.alpha_network = NeuralNetwork(input_dim, output_dim, layer_dim, num_hidden, reg_param, reg_power, ran_seed)\n",
    "\n",
    "        self.balancing_parameter = balancing_parameter\n",
    "\n",
    "\n",
    "    def compute_pde_residual(self, input_distance):\n",
    "            '''\n",
    "            Enforces PDE via automatic differentiation.\n",
    "            Uses NN to predict intensity and absorption, and calculates dI/ds.\n",
    "            '''\n",
    "            input_distance.requires_grad = True\n",
    "\n",
    "            # Predict intensity and absorption coefficient\n",
    "            pred_intensity = self.intensity_network(input_distance)\n",
    "            emission = self.emission_network(input_distance)\n",
    "            alpha = self.alpha_network(input_intensity)\n",
    "\n",
    "            # Compute derivative dI/ds using autograd\n",
    "            dintensity = torch.autograd.grad(pred_intensity.sum(), input_distance, create_graph=True)[0]\n",
    "\n",
    "            # PDE residual: dI/ds + alpha * I = 0\n",
    "            residual = dintensity + alpha * pred_intensity - emission\n",
    "\n",
    "            return residual.reshape(-1,)\n",
    "    \n",
    "    def compute_loss(self, dist_bound, int_bound, input_distance, input_intensity, verbose=True):\n",
    "        '''\n",
    "        Computes loss using boundary conditions, physics residuals, and data loss.\n",
    "        '''\n",
    "        # Predict intensity at boundaries\n",
    "        #pred_initial_intensity = self.intensity_network(dist_bound[0])\n",
    "        pred_boundary_intensity = self.intensity_network(dist_bound)\n",
    "\n",
    "        # Compute PDE residuals (physics enforcement)\n",
    "        physics_residual = self.compute_pde_residual(input_distance)\n",
    "\n",
    "        # **Hard enforce boundary conditions**\n",
    "        #boundary_loss = torch.mean((pred_initial_intensity - int_bound[0])**2)\n",
    "\n",
    "        boundary_loss = torch.mean((pred_boundary_intensity - int_bound)**2)\n",
    "\n",
    "        # Compute data residuals inside the slab\n",
    "        pred_intensity = self.intensity_network(input_distance)\n",
    "        data_residual = input_intensity - pred_intensity\n",
    "\n",
    "        # Compute loss terms (L2 loss)\n",
    "        physics_loss = torch.mean(abs(physics_residual)**2)\n",
    "        data_loss = torch.mean(abs(data_residual)**2)\n",
    "\n",
    "        # Add regularization terms\n",
    "        intensity_reg = self.intensity_network.regularization()\n",
    "        emission_reg = self.emission_network.regularization()\n",
    "        \n",
    "\n",
    "        # Total loss (weighted combination)\n",
    "        total_loss = torch.log(self.balancing_parameter * (data_loss+boundary_loss) + physics_loss + intensity_reg + emission_reg)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Total Loss: {total_loss.item():.4f} | PDE Loss: {torch.log10(physics_loss).item():.4f} | \"\n",
    "                f\"Boundary Loss: {torch.log10(boundary_loss).item():.4f} | Data Loss: {torch.log10(data_loss).item():.4f}\")\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "\n",
    "# Define the function for the analytical solution\n",
    "def analytical_solution(input_distance, emission_value,alpha_value, I0=0.0):\n",
    "    return emission_value/alpha_value +(I0 - emission_value/alpha_value) * torch.exp(-alpha_value * input_distance)\n",
    "\n",
    "def plot_results(model, input_distance, analytical_solution, emission_value):\n",
    "    # Predict intensity from the trained model\n",
    "    pred_intensity = model.intensity_network(input_distance).detach()\n",
    "\n",
    "    # Compute the analytical solution using the true alpha value\n",
    "    anal_intensity = analytical_solution(input_distance, emission_value)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(input_distance.detach().numpy(), pred_intensity.detach().numpy(), label='Predicted Intensity', color='b')\n",
    "    plt.plot(input_distance.detach().numpy(), anal_intensity.detach().numpy(), label='Analytical Intensity', color='r', linestyle='--')\n",
    "    plt.xlabel('Distance (s)')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.legend()\n",
    "    plt.title('Intensity Prediction vs Analytical Solution')\n",
    "    plt.show()\n",
    "\n",
    "    pred_emission = model.emission_network(input_distance).detach().numpy().flatten()\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(input_distance.detach().numpy(), pred_emission, label='Predicted emission', color='b')\n",
    "    plt.axhline(y=emission_value, color='r', linestyle='--', label=f'True emission ({emission_value})')\n",
    "    plt.xlabel('Distance (s)')\n",
    "    plt.ylabel('emission (j)')\n",
    "    plt.legend()\n",
    "    plt.title('Predicted vs Analytical emission')\n",
    "    plt.show()\n",
    "    pred_alpha = model.emission_network(input_distance).detach().numpy().flatten()\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(input_distance.detach().numpy(), pred_alpha, label='Predicted emission', color='b')\n",
    "    plt.axhline(y=alpha_value, color='r', linestyle='--', label=f'True emission ({emission_value})')\n",
    "    plt.xlabel('Distance (s)')\n",
    "    plt.ylabel('emission (j)')\n",
    "    plt.legend()\n",
    "    plt.title('Predicted vs Analytical emission')\n",
    "    plt.show()\n",
    "\n",
    "def fit(model, optimizer, dist_bound, int_bound, input_distance, input_intensity, epochs):\n",
    "    '''\n",
    "    Fits the PINN model using Adam optimization.\n",
    "    '''\n",
    "    history = []  # Store loss values\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(dist_bound, int_bound, input_distance, input_intensity)\n",
    "            loss.backward()\n",
    "            history.append(loss.item())  # Store loss at each epoch\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {history[-1]:.6f}\")\n",
    "\n",
    "    print(f\"Final Loss: {history[-1]:.6f}\")\n",
    "\n",
    "def calculate_relative_l2_error(model, input_distance, analytical_solution, emission_value):\n",
    "    \"\"\"\n",
    "    Calculates the relative L2 error between the predicted intensity and the analytical solution as a percentage.\n",
    "    \"\"\"\n",
    "    # Predict intensity from the trained model\n",
    "    pred_intensity = model.intensity_network(input_distance).detach().flatten()\n",
    "\n",
    "    # Compute the analytical solution using the true alpha value\n",
    "    anal_intensity = analytical_solution(input_distance, emission_value).flatten()\n",
    "\n",
    "    # Compute the L2 norm of the difference between predicted and analytical solutions\n",
    "    numerator = torch.norm(pred_intensity - anal_intensity, p=2)\n",
    "\n",
    "    # Compute the L2 norm of the analytical solution\n",
    "    denominator = torch.norm(anal_intensity, p=2)\n",
    "\n",
    "    # Compute the relative L2 error as a percentage\n",
    "    relative_l2_error = (numerator / denominator) * 100\n",
    "\n",
    "    return relative_l2_error.item()\n",
    "\n",
    "# ==== Initialize Model and Training ==== \n",
    "balancing_parameter = 5.0  # Adjust this if needed\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "layer_dim = 20\n",
    "num_hidden = 6\n",
    "reg_param = 0.001\n",
    "reg_power = 1\n",
    "ran_seed = 32\n",
    "\n",
    "model = PINN(balancing_parameter, input_dim, output_dim, layer_dim, num_hidden, reg_param, reg_power, ran_seed)\n",
    "\n",
    "optimizer_Adam = optim.Adam(\n",
    "    list(model.intensity_network.parameters()) + list(model.emission_network.parameters()),\n",
    "    lr=1e-3,  # Adam optimizer typically uses learning rates like this\n",
    "    betas=(0.5, 0.999),  # Lower momentum for better convergence\n",
    "    eps=1e-8,  # Small value to prevent division by zero\n",
    "    weight_decay=0  # LBFGS doesn't typically use weight decay, so keep it 0\n",
    ")\n",
    "\n",
    "# Define LBFGS optimizer\n",
    "optimizer_LBFGS = optim.LBFGS(\n",
    "    list(model.intensity_network.parameters()) + list(model.emission_network.parameters()),\n",
    "    lr=1e-3,\n",
    "    max_iter=1000,\n",
    "    max_eval=50000,\n",
    "    history_size=150,\n",
    "    line_search_fn=\"strong_wolfe\",\n",
    "    tolerance_change=np.finfo(float).eps\n",
    ")\n",
    "\n",
    "# ==== Training Data (Example Boundaries and Grid Points) ==== \n",
    "I0 = 0.0  # Initial intensity\n",
    "emission_value = 2.0  # Absorption coefficient\n",
    "alpha_value = 0.5\n",
    "\n",
    "dist_bound = torch.tensor([[0.0], [10.0]])  # Slab distance from 0 to 10\n",
    "int_bound = torch.tensor([[I0], [emission_value/alpha_value +(I0 - emission_value/alpha_value)*torch.exp(-alpha_value*10)]])  # Intensity at boundaries\n",
    "\n",
    "x_min = dist_bound.min().item()\n",
    "x_max = dist_bound.max().item()\n",
    "input_distance = torch.linspace(x_min, x_max, 100).view(-1, 1)  # Interior grid\n",
    "\n",
    "input_intensity = analytical_solution(input_distance, emission_value, alpha_value)\n",
    "\n",
    "# Train the model\n",
    "fit(model, optimizer_LBFGS, dist_bound, int_bound, input_distance, input_intensity, epochs=50)\n",
    "\n",
    "# Calculate relative L2 error as a percentage\n",
    "relative_l2_error = calculate_relative_l2_error(model, input_distance, analytical_solution, emission_value)\n",
    "print(f\"Relative L2 Error: {relative_l2_error:.6f}%\")\n",
    "\n",
    "# Plot the results **only once** after training\n",
    "plot_results(model, input_distance, analytical_solution,emission_value, alpha_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
